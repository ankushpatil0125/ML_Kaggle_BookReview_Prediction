{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Preprocessing of Data***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1dsFg21QcYUA"
      },
      "id": "1dsFg21QcYUA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import the Libraries**"
      ],
      "metadata": {
        "id": "uF6Y0Y7DaY32"
      },
      "id": "uF6Y0Y7DaY32"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adb6c6ea",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2023-11-25T17:12:34.766767Z",
          "iopub.status.busy": "2023-11-25T17:12:34.766424Z",
          "iopub.status.idle": "2023-11-25T17:12:35.584308Z",
          "shell.execute_reply": "2023-11-25T17:12:35.582940Z"
        },
        "papermill": {
          "duration": 0.828916,
          "end_time": "2023-11-25T17:12:35.586636",
          "exception": false,
          "start_time": "2023-11-25T17:12:34.757720",
          "status": "completed"
        },
        "tags": [],
        "id": "adb6c6ea",
        "outputId": "5714f85a-8c54-48a8-e1a6-940fa033599b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/input/train-data/train.csv\n"
          ]
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read CSV file**"
      ],
      "metadata": {
        "id": "zCkRwJduafQS"
      },
      "id": "zCkRwJduafQS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd95cc4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:12:35.602683Z",
          "iopub.status.busy": "2023-11-25T17:12:35.602312Z",
          "iopub.status.idle": "2023-11-25T17:12:51.088665Z",
          "shell.execute_reply": "2023-11-25T17:12:51.087831Z"
        },
        "papermill": {
          "duration": 15.497131,
          "end_time": "2023-11-25T17:12:51.091135",
          "exception": false,
          "start_time": "2023-11-25T17:12:35.594004",
          "status": "completed"
        },
        "tags": [],
        "id": "0fd95cc4"
      },
      "outputs": [],
      "source": [
        "path_link=\"/kaggle/input/train-data/train.csv\"\n",
        "df=pd.read_csv(path_link)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "w9TlrZ72aj4e"
      },
      "id": "w9TlrZ72aj4e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f70a747",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:12:51.107875Z",
          "iopub.status.busy": "2023-11-25T17:12:51.107162Z",
          "iopub.status.idle": "2023-11-25T17:12:51.232073Z",
          "shell.execute_reply": "2023-11-25T17:12:51.231274Z"
        },
        "papermill": {
          "duration": 0.135384,
          "end_time": "2023-11-25T17:12:51.234400",
          "exception": false,
          "start_time": "2023-11-25T17:12:51.099016",
          "status": "completed"
        },
        "tags": [],
        "id": "0f70a747"
      },
      "outputs": [],
      "source": [
        "columns_to_keep = ['review_text', 'rating']\n",
        "\n",
        "# Drop all columns except the specified ones\n",
        "df = df[columns_to_keep]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Remove Urls**"
      ],
      "metadata": {
        "id": "BesyJv_lanyX"
      },
      "id": "BesyJv_lanyX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d863ed",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:12:51.250724Z",
          "iopub.status.busy": "2023-11-25T17:12:51.250014Z",
          "iopub.status.idle": "2023-11-25T17:13:09.995125Z",
          "shell.execute_reply": "2023-11-25T17:13:09.994162Z"
        },
        "papermill": {
          "duration": 18.755774,
          "end_time": "2023-11-25T17:13:09.997462",
          "exception": false,
          "start_time": "2023-11-25T17:12:51.241688",
          "status": "completed"
        },
        "tags": [],
        "id": "c0d863ed",
        "outputId": "5269328d-2b39-4144-e668-7ccc15110ab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review_text\n",
            "0       My only complaint is this isn't a trilogy...I ...\n",
            "1       i read this 2 days ago.... and remember nothin...\n",
            "2       I wasn't a fan of the first Monster Blood, and...\n",
            "3       Love Theo so much. The interaction between she...\n",
            "4       a good book, just had to get through the middl...\n",
            "...                                                   ...\n",
            "629995  **edit 11/26/13 \\n My advice regarding the Vor...\n",
            "629996  This is not a book I would likely have picked ...\n",
            "629997  This book's summary boasts a spectacular story...\n",
            "629998  Excellent sequel to Wolf Hall- covering a shor...\n",
            "629999  Such a beautifully, complicated, different sto...\n",
            "\n",
            "[630000 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "# Apply the function to the 'review_text' column\n",
        "df['review_text'] = df['review_text'].apply(remove_urls)\n",
        "\n",
        "# Display the DataFrame after removing URLs\n",
        "print(df[['review_text']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9760d25",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:13:10.014453Z",
          "iopub.status.busy": "2023-11-25T17:13:10.013689Z",
          "iopub.status.idle": "2023-11-25T17:13:28.791536Z",
          "shell.execute_reply": "2023-11-25T17:13:28.790468Z"
        },
        "papermill": {
          "duration": 18.788041,
          "end_time": "2023-11-25T17:13:28.793682",
          "exception": false,
          "start_time": "2023-11-25T17:13:10.005641",
          "status": "completed"
        },
        "tags": [],
        "id": "e9760d25",
        "outputId": "48a2ff89-5cd9-43fb-d98c-c047777e9477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review_text\n",
            "0       My only complaint is this isn't a trilogy...I ...\n",
            "1       i read this 2 days ago.... and remember nothin...\n",
            "2       I wasn't a fan of the first Monster Blood, and...\n",
            "3       Love Theo so much. The interaction between she...\n",
            "4       a good book, just had to get through the middl...\n",
            "...                                                   ...\n",
            "629995  **edit 11/26/13 \\n My advice regarding the Vor...\n",
            "629996  This is not a book I would likely have picked ...\n",
            "629997  This book's summary boasts a spectacular story...\n",
            "629998  Excellent sequel to Wolf Hall- covering a shor...\n",
            "629999  Such a beautifully, complicated, different sto...\n",
            "\n",
            "[630000 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "# Apply the function to the 'review_text' column\n",
        "df['review_text'] = df['review_text'].apply(remove_urls)\n",
        "\n",
        "# Display the DataFrame after removing URLs\n",
        "print(df[['review_text']])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Remove newline character**"
      ],
      "metadata": {
        "id": "iGK_fQSSbKdD"
      },
      "id": "iGK_fQSSbKdD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ea1026d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:13:28.810206Z",
          "iopub.status.busy": "2023-11-25T17:13:28.809870Z",
          "iopub.status.idle": "2023-11-25T17:13:30.937993Z",
          "shell.execute_reply": "2023-11-25T17:13:30.937084Z"
        },
        "papermill": {
          "duration": 2.139063,
          "end_time": "2023-11-25T17:13:30.940491",
          "exception": false,
          "start_time": "2023-11-25T17:13:28.801428",
          "status": "completed"
        },
        "tags": [],
        "id": "2ea1026d",
        "outputId": "7a9011d1-49ba-4a26-cb33-479a1b5372bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0         My only complaint is this isn't a trilogy...I ...\n",
            "1         i read this 2 days ago.... and remember nothin...\n",
            "2         I wasn't a fan of the first Monster Blood, and...\n",
            "3         Love Theo so much. The interaction between she...\n",
            "4         a good book, just had to get through the middl...\n",
            "                                ...                        \n",
            "629995    **edit 11/26/13  My advice regarding the Vorko...\n",
            "629996    This is not a book I would likely have picked ...\n",
            "629997    This book's summary boasts a spectacular story...\n",
            "629998    Excellent sequel to Wolf Hall- covering a shor...\n",
            "629999    Such a beautifully, complicated, different sto...\n",
            "Name: review_text, Length: 630000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def remove_newlines_regex(text):\n",
        "    # Use a regular expression to match newline characters and replace them with an empty string\n",
        "    cleaned_text = re.sub(r'\\n', '', text)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Example text with newline characters\n",
        "df['review_text']=df['review_text'].apply(remove_newlines_regex)\n",
        "print(df['review_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Remove Dates**"
      ],
      "metadata": {
        "id": "k8Nf2yd4bPzC"
      },
      "id": "k8Nf2yd4bPzC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315574cd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:13:30.962403Z",
          "iopub.status.busy": "2023-11-25T17:13:30.962040Z",
          "iopub.status.idle": "2023-11-25T17:14:32.202711Z",
          "shell.execute_reply": "2023-11-25T17:14:32.201543Z"
        },
        "papermill": {
          "duration": 61.263813,
          "end_time": "2023-11-25T17:14:32.211528",
          "exception": false,
          "start_time": "2023-11-25T17:13:30.947715",
          "status": "completed"
        },
        "tags": [],
        "id": "315574cd",
        "outputId": "15edce13-7c38-4c5c-9619-a48a31560ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0         My only complaint is this isn't a trilogy...I ...\n",
            "1         i read this 2 days ago.... and remember nothin...\n",
            "2         I wasn't a fan of the first Monster Blood, and...\n",
            "3         Love Theo so much. The interaction between she...\n",
            "4         a good book, just had to get through the middl...\n",
            "                                ...                        \n",
            "629995    **edit   My advice regarding the Vorkosigan Sa...\n",
            "629996    This is not a book I would likely have picked ...\n",
            "629997    This book's summary boasts a spectacular story...\n",
            "629998    Excellent sequel to Wolf Hall- covering a shor...\n",
            "629999    Such a beautifully, complicated, different sto...\n",
            "Name: review_text, Length: 630000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def remove_dates(text):\n",
        "    # Regular expression to match dates in the format YYYY-MM-DD or DD/MM/YY or DD/MM/YYYY\n",
        "    date_pattern = r'\\b\\d{4}-\\d{2}-\\d{2}\\b|\\b\\d{2}/\\d{2}/\\d{2}\\b|\\b\\d{2}/\\d{2}/\\d{4}\\b|\\b\\d{1,2} (?:january|february|march|april|may|june|july|august|september|october|november|december) \\d{4}\\b'\n",
        "\n",
        "    # Remove dates from the text\n",
        "    return re.sub(date_pattern, '', text)\n",
        "\n",
        "df['review_text'] = df['review_text'].apply(lambda x: remove_dates(x))\n",
        "print(df['review_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Lowercasing the review text**"
      ],
      "metadata": {
        "id": "8OBMsihfbTE9"
      },
      "id": "8OBMsihfbTE9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7381380",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:14:32.227519Z",
          "iopub.status.busy": "2023-11-25T17:14:32.227209Z",
          "iopub.status.idle": "2023-11-25T17:14:33.031433Z",
          "shell.execute_reply": "2023-11-25T17:14:33.030357Z"
        },
        "papermill": {
          "duration": 0.814931,
          "end_time": "2023-11-25T17:14:33.033819",
          "exception": false,
          "start_time": "2023-11-25T17:14:32.218888",
          "status": "completed"
        },
        "tags": [],
        "id": "c7381380",
        "outputId": "035e8ea1-6c56-4733-c4bf-5385697fdbe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0         my only complaint is this isn't a trilogy...i ...\n",
            "1         i read this 2 days ago.... and remember nothin...\n",
            "2         i wasn't a fan of the first monster blood, and...\n",
            "3         love theo so much. the interaction between she...\n",
            "4         a good book, just had to get through the middl...\n",
            "                                ...                        \n",
            "629995    **edit   my advice regarding the vorkosigan sa...\n",
            "629996    this is not a book i would likely have picked ...\n",
            "629997    this book's summary boasts a spectacular story...\n",
            "629998    excellent sequel to wolf hall- covering a shor...\n",
            "629999    such a beautifully, complicated, different sto...\n",
            "Name: review_text, Length: 630000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "df['review_text']=df['review_text'].str.lower()\n",
        "print(df['review_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Expanding Contractions**"
      ],
      "metadata": {
        "id": "xQixnJ4vbZVC"
      },
      "id": "xQixnJ4vbZVC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5528800d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:14:33.051240Z",
          "iopub.status.busy": "2023-11-25T17:14:33.050935Z",
          "iopub.status.idle": "2023-11-25T17:15:54.003502Z",
          "shell.execute_reply": "2023-11-25T17:15:54.002478Z"
        },
        "papermill": {
          "duration": 80.971657,
          "end_time": "2023-11-25T17:15:54.013683",
          "exception": false,
          "start_time": "2023-11-25T17:14:33.042026",
          "status": "completed"
        },
        "tags": [],
        "id": "5528800d",
        "outputId": "8d1ead24-7fb0-4fcd-caaf-d249bcee8f88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\r\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\r\n",
            "Collecting textsearch>=0.0.21 (from contractions)\r\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\r\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\r\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\r\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\r\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\r\n",
            "0         my only complaint is this is not a trilogy...i...\n",
            "1         i read this 2 days ago.... and remember nothin...\n",
            "2         i was not a fan of the first monster blood, an...\n",
            "3         love theo so much. the interaction between she...\n",
            "4         a good book, just had to get through the middl...\n",
            "                                ...                        \n",
            "629995    **edit   my advice regarding the vorkosigan sa...\n",
            "629996    this is not a book i would likely have picked ...\n",
            "629997    this book's summary boasts a spectacular story...\n",
            "629998    excellent sequel to wolf hall- covering a shor...\n",
            "629999    such a beautifully, complicated, different sto...\n",
            "Name: review_text, Length: 630000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    # Use the contractions library to expand contractions\n",
        "    expanded_text = contractions.fix(text)\n",
        "    return expanded_text\n",
        "df['review_text'] = df['review_text'].apply(lambda x: expand_contractions(x))\n",
        "print(df['review_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Remove Punctuations**"
      ],
      "metadata": {
        "id": "vt-kQzrjbfd2"
      },
      "id": "vt-kQzrjbfd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "399862cc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:15:54.037212Z",
          "iopub.status.busy": "2023-11-25T17:15:54.036647Z",
          "iopub.status.idle": "2023-11-25T17:15:54.061525Z",
          "shell.execute_reply": "2023-11-25T17:15:54.060621Z"
        },
        "papermill": {
          "duration": 0.041484,
          "end_time": "2023-11-25T17:15:54.063668",
          "exception": false,
          "start_time": "2023-11-25T17:15:54.022184",
          "status": "completed"
        },
        "tags": [],
        "id": "399862cc"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf77341e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:15:54.082665Z",
          "iopub.status.busy": "2023-11-25T17:15:54.082076Z",
          "iopub.status.idle": "2023-11-25T17:15:54.086582Z",
          "shell.execute_reply": "2023-11-25T17:15:54.085580Z"
        },
        "papermill": {
          "duration": 0.015943,
          "end_time": "2023-11-25T17:15:54.088555",
          "exception": false,
          "start_time": "2023-11-25T17:15:54.072612",
          "status": "completed"
        },
        "tags": [],
        "id": "bf77341e"
      },
      "outputs": [],
      "source": [
        "def remove_punc1(text):\n",
        "    return text.translate(str.maketrans('','',string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cfb063f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:15:54.106209Z",
          "iopub.status.busy": "2023-11-25T17:15:54.105951Z",
          "iopub.status.idle": "2023-11-25T17:15:59.099304Z",
          "shell.execute_reply": "2023-11-25T17:15:59.098538Z"
        },
        "papermill": {
          "duration": 5.00489,
          "end_time": "2023-11-25T17:15:59.101818",
          "exception": false,
          "start_time": "2023-11-25T17:15:54.096928",
          "status": "completed"
        },
        "tags": [],
        "id": "1cfb063f"
      },
      "outputs": [],
      "source": [
        "df['review_text']=df['review_text'].apply(remove_punc1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "533e6d15",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:15:59.120639Z",
          "iopub.status.busy": "2023-11-25T17:15:59.120366Z",
          "iopub.status.idle": "2023-11-25T17:16:11.219746Z",
          "shell.execute_reply": "2023-11-25T17:16:11.218386Z"
        },
        "papermill": {
          "duration": 12.111391,
          "end_time": "2023-11-25T17:16:11.222001",
          "exception": false,
          "start_time": "2023-11-25T17:15:59.110610",
          "status": "completed"
        },
        "tags": [],
        "id": "533e6d15",
        "outputId": "5ca8f4f7-c528-48d4-ae42-419e42c58c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\r\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\r\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.Remove Stopwords**"
      ],
      "metadata": {
        "id": "VGg3SNeLb_5N"
      },
      "id": "VGg3SNeLb_5N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "151e3450",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:16:11.240949Z",
          "iopub.status.busy": "2023-11-25T17:16:11.240598Z",
          "iopub.status.idle": "2023-11-25T17:32:27.683225Z",
          "shell.execute_reply": "2023-11-25T17:32:27.682410Z"
        },
        "papermill": {
          "duration": 976.454903,
          "end_time": "2023-11-25T17:32:27.685832",
          "exception": false,
          "start_time": "2023-11-25T17:16:11.230929",
          "status": "completed"
        },
        "tags": [],
        "id": "151e3450",
        "outputId": "3ccaa8c7-3174-4017-cbdf-f801e6aadc84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Get the list of English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove stop words from text\n",
        "def remove_stop_words(text):\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply the function to the 'review_text' column\n",
        "df['review_text'] = df['review_text'].apply(remove_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2f5c378",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:32:27.704586Z",
          "iopub.status.busy": "2023-11-25T17:32:27.704291Z",
          "iopub.status.idle": "2023-11-25T17:32:27.725122Z",
          "shell.execute_reply": "2023-11-25T17:32:27.724260Z"
        },
        "papermill": {
          "duration": 0.032777,
          "end_time": "2023-11-25T17:32:27.727084",
          "exception": false,
          "start_time": "2023-11-25T17:32:27.694307",
          "status": "completed"
        },
        "tags": [],
        "id": "d2f5c378"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(punctuation)\n",
        "stop.update(punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **POS Tagging and Remove Repeated words**"
      ],
      "metadata": {
        "id": "wMqJXf7gcHZX"
      },
      "id": "wMqJXf7gcHZX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6fcea2b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:32:27.744506Z",
          "iopub.status.busy": "2023-11-25T17:32:27.744254Z",
          "iopub.status.idle": "2023-11-25T17:32:27.849550Z",
          "shell.execute_reply": "2023-11-25T17:32:27.848535Z"
        },
        "papermill": {
          "duration": 0.116267,
          "end_time": "2023-11-25T17:32:27.851552",
          "exception": false,
          "start_time": "2023-11-25T17:32:27.735285",
          "status": "completed"
        },
        "tags": [],
        "id": "c6fcea2b",
        "outputId": "ab7496d1-d6fd-443d-9b9e-35b7b1747921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /usr/share/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ae6894",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:32:27.871190Z",
          "iopub.status.busy": "2023-11-25T17:32:27.870878Z",
          "iopub.status.idle": "2023-11-25T17:33:00.037110Z",
          "shell.execute_reply": "2023-11-25T17:33:00.036038Z"
        },
        "papermill": {
          "duration": 32.178805,
          "end_time": "2023-11-25T17:33:00.039674",
          "exception": false,
          "start_time": "2023-11-25T17:32:27.860869",
          "status": "completed"
        },
        "tags": [],
        "id": "02ae6894",
        "outputId": "7da1f512-9c94-441a-c6b6-9ddb5c5599ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting symspellpy\r\n",
            "  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25hCollecting editdistpy>=0.1.3 (from symspellpy)\r\n",
            "  Downloading editdistpy-0.1.3.tar.gz (57 kB)\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
            "\u001b[?25hBuilding wheels for collected packages: editdistpy\r\n",
            "  Building wheel for editdistpy (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
            "\u001b[?25h  Created wheel for editdistpy: filename=editdistpy-0.1.3-cp310-cp310-linux_x86_64.whl size=47886 sha256=bbc4c671c181d518ddd8cfa43e3bc516661e6b73cb361de1ee6d0603b66862c4\r\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/6a/a6/a1283cc145323a1fb3d475bd158ee60b248ab1985230d266fc\r\n",
            "Successfully built editdistpy\r\n",
            "Installing collected packages: editdistpy, symspellpy\r\n",
            "Successfully installed editdistpy-0.1.3 symspellpy-6.7.7\r\n"
          ]
        }
      ],
      "source": [
        "# installing the package and importing the required libraries\n",
        "!pip install -U symspellpy\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity # efficient spelling correction\n",
        "\n",
        "# using the dictionary of enlish words\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96e04ed",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:33:00.067118Z",
          "iopub.status.busy": "2023-11-25T17:33:00.066778Z",
          "iopub.status.idle": "2023-11-25T17:33:00.582265Z",
          "shell.execute_reply": "2023-11-25T17:33:00.581051Z"
        },
        "papermill": {
          "duration": 0.532477,
          "end_time": "2023-11-25T17:33:00.584468",
          "exception": false,
          "start_time": "2023-11-25T17:33:00.051991",
          "status": "completed"
        },
        "tags": [],
        "id": "a96e04ed",
        "outputId": "93c4d8bd-edec-4c15-fd93-76e239373548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /kaggle/working/...\n",
            "Archive:  /kaggle/working/corpora/wordnet.zip\n",
            "   creating: /kaggle/working/corpora/wordnet/\n",
            "  inflating: /kaggle/working/corpora/wordnet/lexnames  \n",
            "  inflating: /kaggle/working/corpora/wordnet/data.verb  \n",
            "  inflating: /kaggle/working/corpora/wordnet/index.adv  \n",
            "  inflating: /kaggle/working/corpora/wordnet/adv.exc  \n",
            "  inflating: /kaggle/working/corpora/wordnet/index.verb  \n",
            "  inflating: /kaggle/working/corpora/wordnet/cntlist.rev  \n",
            "  inflating: /kaggle/working/corpora/wordnet/data.adj  \n",
            "  inflating: /kaggle/working/corpora/wordnet/index.adj  \n",
            "  inflating: /kaggle/working/corpora/wordnet/LICENSE  \n",
            "  inflating: /kaggle/working/corpora/wordnet/citation.bib  \n",
            "  inflating: /kaggle/working/corpora/wordnet/noun.exc  \n",
            "  inflating: /kaggle/working/corpora/wordnet/verb.exc  \n",
            "  inflating: /kaggle/working/corpora/wordnet/README  \n",
            "  inflating: /kaggle/working/corpora/wordnet/index.sense  \n",
            "  inflating: /kaggle/working/corpora/wordnet/data.noun  \n",
            "  inflating: /kaggle/working/corpora/wordnet/data.adv  \n",
            "  inflating: /kaggle/working/corpora/wordnet/index.noun  \n",
            "  inflating: /kaggle/working/corpora/wordnet/adj.exc  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import subprocess\n",
        "\n",
        "# Download and unzip wordnet\n",
        "try:\n",
        "    nltk.data.find('wordnet.zip')\n",
        "except:\n",
        "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
        "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
        "    subprocess.run(command.split())\n",
        "    nltk.data.path.append('/kaggle/working/')\n",
        "\n",
        "# Now you can import the NLTK resources as usual\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32461d5e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:33:00.653004Z",
          "iopub.status.busy": "2023-11-25T17:33:00.652642Z",
          "iopub.status.idle": "2023-11-25T17:53:23.842851Z",
          "shell.execute_reply": "2023-11-25T17:53:23.841976Z"
        },
        "papermill": {
          "duration": 1223.24985,
          "end_time": "2023-11-25T17:53:23.846140",
          "exception": false,
          "start_time": "2023-11-25T17:33:00.596290",
          "status": "completed"
        },
        "tags": [],
        "id": "32461d5e"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet  # to replace repeated words\n",
        "import re\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "class RepeatReplacer(object):\n",
        "    def __init__(self):\n",
        "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "        self.repl = r'\\1\\2\\3'\n",
        "\n",
        "    def replace(self, word):\n",
        "        if wordnet.synsets(word):\n",
        "            return word\n",
        "\n",
        "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
        "\n",
        "        if repl_word != word:\n",
        "            return self.replace(repl_word)\n",
        "        else:\n",
        "            return repl_word\n",
        "\n",
        "# instantiating the sym spell object\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "\n",
        "# loading the dictionary to match to the correct words\n",
        "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# instantiating the object for repeated character replacement\n",
        "replacer = RepeatReplacer()\n",
        "\n",
        "# function to correct misspelled words\n",
        "def spellingCorrectionSymSpell(text):\n",
        "    ans = \"\"\n",
        "    for word in text.split():\n",
        "        suggestions = sym_spell.lookup(word, Verbosity.TOP, max_edit_distance=2, include_unknown=False)\n",
        "\n",
        "        if not suggestions:\n",
        "            corrected_word = replacer.replace(word)\n",
        "            suggestions = sym_spell.lookup(corrected_word, Verbosity.TOP, max_edit_distance=2, include_unknown=True)\n",
        "\n",
        "        if suggestions:\n",
        "            corrected_word = str(suggestions[0])\n",
        "            match = re.search(r'[a-zA-Z]+', corrected_word)\n",
        "            if match:\n",
        "                ans += match.group() + \" \"\n",
        "\n",
        "    return ans[:-1]\n",
        "\n",
        "# X_train_df['Reviews'] = X_train_df['Reviews'].apply(spellingCorrectionSymSpell)\n",
        "df['review_text'] = df['review_text'].apply(spellingCorrectionSymSpell)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3653e996",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:53:23.872043Z",
          "iopub.status.busy": "2023-11-25T17:53:23.871432Z",
          "iopub.status.idle": "2023-11-25T17:53:23.895661Z",
          "shell.execute_reply": "2023-11-25T17:53:23.894836Z"
        },
        "papermill": {
          "duration": 0.03913,
          "end_time": "2023-11-25T17:53:23.897586",
          "exception": false,
          "start_time": "2023-11-25T17:53:23.858456",
          "status": "completed"
        },
        "tags": [],
        "id": "3653e996"
      },
      "outputs": [],
      "source": [
        "def get_simple_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmitization & Tokenization**"
      ],
      "metadata": {
        "id": "avawkp-LcOz1"
      },
      "id": "avawkp-LcOz1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae8ae95",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:53:23.921727Z",
          "iopub.status.busy": "2023-11-25T17:53:23.921224Z",
          "iopub.status.idle": "2023-11-25T17:53:23.950911Z",
          "shell.execute_reply": "2023-11-25T17:53:23.950021Z"
        },
        "papermill": {
          "duration": 0.043821,
          "end_time": "2023-11-25T17:53:23.952895",
          "exception": false,
          "start_time": "2023-11-25T17:53:23.909074",
          "status": "completed"
        },
        "tags": [],
        "id": "aae8ae95",
        "outputId": "31ca31e0-c2f9-4af0-f02c-854563141e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Function to clean our text.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Make sure to download the WordNet data\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_review(text):\n",
        "    clean_text = []\n",
        "    for w in word_tokenize(text):\n",
        "        if w.lower() not in stop:\n",
        "            pos = pos_tag([w])\n",
        "            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n",
        "            clean_text.append(new_w)\n",
        "    return clean_text\n",
        "\n",
        "def join_text(text):\n",
        "    return \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe46fe4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T17:53:23.977144Z",
          "iopub.status.busy": "2023-11-25T17:53:23.976365Z",
          "iopub.status.idle": "2023-11-25T21:06:23.300289Z",
          "shell.execute_reply": "2023-11-25T21:06:23.299284Z"
        },
        "papermill": {
          "duration": 11579.338765,
          "end_time": "2023-11-25T21:06:23.303229",
          "exception": false,
          "start_time": "2023-11-25T17:53:23.964464",
          "status": "completed"
        },
        "tags": [],
        "id": "2fe46fe4"
      },
      "outputs": [],
      "source": [
        "df['review_text']=df['review_text'].apply(clean_review)\n",
        "df['review_text']=df['review_text'].apply(join_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataframe to CSV file**"
      ],
      "metadata": {
        "id": "mlaYqyzOcu15"
      },
      "id": "mlaYqyzOcu15"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003216ff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T21:06:23.332317Z",
          "iopub.status.busy": "2023-11-25T21:06:23.331931Z",
          "iopub.status.idle": "2023-11-25T21:06:42.242429Z",
          "shell.execute_reply": "2023-11-25T21:06:42.241579Z"
        },
        "papermill": {
          "duration": 18.928557,
          "end_time": "2023-11-25T21:06:42.244968",
          "exception": false,
          "start_time": "2023-11-25T21:06:23.316411",
          "status": "completed"
        },
        "tags": [],
        "id": "003216ff"
      },
      "outputs": [],
      "source": [
        "df.to_csv('final_preprocessed.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 4058354,
          "sourceId": 7051619,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30588,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 14053.216997,
      "end_time": "2023-11-25T21:06:44.287372",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-11-25T17:12:31.070375",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}